# Scheduler y Gestión de Memoria Virtual - Sistema Operativo Rust ARMv8

El kernel de RavnOS combina un scheduler preventivo por núcleo con un subsistema de memoria basado en páginas de 64 KiB. Este documento describe el estado real del código en `source_code/kernel/src`, corrige especificaciones obsoletas y explica los fundamentos para que cualquier persona pueda entender el diseño.

## Términos

1. **AArch64:** Conjunto de instrucciones de 64 bits de ARM usado por la Raspberry Pi 4/5 y por QEMU en este proyecto.
2. **PCB (Process Control Block):** Estructura que almacena todo el contexto de un hilo/proceso (registros, estado, stacks, mailboxes, métricas).
3. **TTBR (Translation Table Base Register):** Registro que apunta a la raíz de una jerarquía de tablas de páginas. ARM define TTBR0_EL1 (user) y TTBR1_EL1 (kernel).
4. **TTBR0_EL1:** Tabla de páginas usada cuando el procesador accede a direcciones de usuario (EL0).
5. **TTBR1_EL1:** Tabla de páginas usada al ejecutar código del kernel (EL1).
6. **EL0 / EL1 / EL2:** “Exception Levels” de ARM. EL0 es userland, EL1 es kernel, EL2 es hypervisor. El boot pasa de EL2 a EL1 antes de entrar a Rust.
7. **GIC (Generic Interrupt Controller):** Controlador ARM que recibe interrupciones y decide qué núcleo las ejecuta.
8. **PPI (Private Peripheral Interrupt):** Interrupción privada para cada CPU. El temporizador genérico usa el PPI #14 (INTID 30).
9. **IPI / SGI (Inter-Processor Interrupt / Software Generated Interrupt):** Señales enviadas entre núcleos mediante el GIC para sincronización (por ejemplo, invalidar TLBs).
10. **MMU (Memory Management Unit):** Hardware que traduce direcciones virtuales a físicas siguiendo las tablas apuntadas por los TTBR.
11. **DMB / DSB / ISB:** Barreras de memoria de ARM. DMB sincroniza accesos de datos, DSB garantiza que todas las escrituras previas finalicen y ISB sincroniza el pipeline de instrucciones.
12. **TLB (Translation Lookaside Buffer):** Caché de traducciones. Debe invalidarse cuando cambian las tablas de páginas.
13. **PTE (Page Table Entry):** Entrada de una tabla de páginas. Define dirección física, atributos y permisos de una página de 64 KiB.
14. **Página guardia:** Página marcada como “Fault” que detecta overflows de stack. El PCB tiene campos reservados para este mecanismo.
15. **DTB (Device Tree Blob):** Descripción de hardware que entrega el bootloader. El kernel la parsea para conocer periféricos y regiones de RAM.
16. **ASLR (Address Space Layout Randomization):** Técnica que cambia las bases de heap/stack para todos los procesos.
17. **Mailbox:** Par de buffers `request/response` dentro del PCB usado para intercambiar syscalls entre un núcleo de usuario y el núcleo 0.
18. **MMIO (Memory-Mapped I/O):** Área de direcciones virtuales/físicas donde viven los periféricos (UART, GIC, GPIO).
19. **UART (Universal Asynchronous Receiver/Transmitter):** Dispositivo de serie usado como consola. Aquí se usa el PL011.
20. **CNTFRQ_EL0:** Registro que informa la frecuencia del temporizador genérico.
21. **CNTPCT_EL0:** Contador libre que incrementa a la frecuencia indicada por CNTFRQ_EL0.
22. **CNTP_CTL_EL0 / CNTP_TVAL_EL0 / CNTP_CVAL_EL0:** Registros del temporizador físico no seguro. Controlan habilitación, countdown y el valor absoluto del siguiente disparo.
23. **DAIF:** Registro que contiene los flags de enmascarado de interrupciones (Debug, SError, IRQ, FIQ).
24. **ContextFrame:** Snapshot de registros generales (x0-x30), SP, ELR y SPSR guardado dentro del PCB.
25. **ExceptionFrame:** Snapshot temporal que el handler en ensamblador coloca en la pila antes de convertirlo en un ContextFrame.
26. **AtomicBool / AtomicPtr / AtomicU64:** Primitivas atómicas de Rust usadas para marcar PCBs reservados, publicar punteros y llevar contadores de telemetría.
27. **MAIR_EL1:** Registro que define los atributos de memoria (Normal WBWA, Device, etc.).
28. **TCR_EL1:** Registro que controla el tamaño del espacio virtual, el granulo y los permisos de caché para TTBR0/TTBR1.
29. **SCTLR_EL1:** Registro que enciende la MMU, las cachés y varias protecciones adicionales.
30. **KernelTables:** Estructura de Rust que encapsula la jerarquía de tablas del kernel y expone métodos `map_identity` y `translate`.
31. **UserAddressSpace:** Tabla de páginas de usuario construida por el gestor de memoria y asociada a cada slot cooperativo (`USER_TASKS`).
32. **PageAllocator:** Reservas de memoria física basadas en segmentos alineados a 64 KiB.
33. **MemorySummary:** Resumen de telemetría (regiones libres, reservas, kernel image, mmio_base).
34. **VBAR_EL1:** Registro que contiene la dirección de la tabla de vectores de excepciones.
35. **IRQ (Interrupt Request):** Evento que interrumpe el flujo normal de ejecución.
36. **NullLock:** Implementación mínima de `Mutex` usada para proteger estructuras globales sin depender aún del scheduler.
37. **PL011:** UART presente en Raspberry Pi. Se usa como consola primaria.
38. **IRQ heartbeat:** Contador global (`AtomicU64`) que guarda el último valor de `CNTPCT_EL0` observado por el handler para facilitar la depuración de temporizadores.

## Teoría

### Arquitectura ARM y niveles de excepción
ARM divide la ejecución en niveles. EL0 corre aplicaciones sin privilegios, EL1 alberga el kernel, y EL2 es un hypervisor opcional. El firmware arranca en EL2; nuestro `boot.s` ejecuta la rutina `el2_to_el1` para habilitar temporizadores en EL1, limpiar offsets de contador y saltar al kernel con privilegios completos. Cada nivel tiene su propio stack y sus propios registros especiales (SP_EL0, SP_EL1, ELR_EL1, SPSR_EL1), de modo que el cambio entre niveles preserva el contexto.

### Memoria virtual con páginas de 64 KiB
La MMU traduce direcciones virtuales a físicas usando tablas jerárquicas. Elegimos granulos de 64 KiB para reducir la presión en las tablas y alinearnos con la documentación del scheduler. TTBR0_EL1 apunta a las tablas de usuario y TTBR1_EL1 al kernel. El registro TCR_EL1 define el tamaño del espacio virtual (48 bits) y el granulo para ambos TTBR. Cuando la MMU está activa (SCTLR_EL1.M=1) cada acceso consulta la tabla adecuada y llena el TLB. Cualquier cambio en una PTE requiere invalidar las entradas asociadas (`TLBI`) y emitir `DSB`/`ISB` para garantizar coherencia.

### Controlador de interrupciones y temporizador genérico
El GIC-400 recibe interrupciones de los dispositivos y las reparte entre los núcleos. Un PPI (Private Peripheral Interrupt) se instancia por CPU; usamos el PPI #14 para el temporizador físico no seguro. Cada core programa su temporizador escribiendo en `CNTP_TVAL_EL0` con el número de ticks (obtenido dividiendo `CNTFRQ_EL0` entre 200 para lograr 5 ms). Cuando el contador llega a cero, el GIC genera una IRQ que hace que el procesador salte a la dirección almacenada en `VBAR_EL1`. Allí nuestro handler salva registros, reprograma el timer y ejecuta el cambio de contexto.

### Sincronización y estructuras atómicas
El kernel evita compartir mutables sin protección. `NullLock` (basado en `Mutex`) simplifica el acceso a tablas globales durante el arranque. Para la parte crítica del scheduler se usan `AtomicBool`, `AtomicPtr` y `AtomicU64`. Los órdenes de memoria (Acquire/Release) garantizan que los núcleos observen estados consistentes sin necesidad de spinlocks pesados. Para operaciones de bajo nivel se usan barreras `DMB`/`DSB`/`ISB` y macros `asm!`.

### Scheduling preventivo y cambio de contexto
Un scheduler preventivo interrumpe periódicamente a cada núcleo, guarda su contexto y decide qué tarea ejecutar después. Aquí cada core es responsable de su propio quantum: el handler `scheduler_irq_handler` guarda un `ExceptionFrame`, llama a `scheduler_preempt_tick` (Rust) para decidir el siguiente `ContextFrame`, reprograma el timer y restaura los registros antes de hacer `eret`. Las tareas del kernel viven en Core 0 y las tareas de usuario en Cores 1..N. Los PCBs conservan el contexto para que el scheduler pueda reanudar cualquier slot.

## Principios de Implementación

1. Todo el código de alto nivel está en Rust; las rutinas críticas (arranque, ahorro/restauración de registros y transición EL2→EL1) viven en ensamblador AArch64 cuidadosamente comentado.
2. El kernel no realiza escrituras globales sin protección. `NullLock`, `Atomic*` y las barreras de ARM mantienen el orden adecuado incluso antes de que exista el scheduler completo.
3. Cada cambio sensible (MMU, IRQ, scheduler) produce telemetría vía `await_kernel_uart_println!`, lo que permite reproducir errores en QEMU o hardware físico.
4. Se prefiere el mapeo 1:1 en TTBR1 para simplificar el bring-up inicial. Las tablas de usuario son identitarias solo mientras no existan procesos reales; sus TTBR se cargan dinámicamente.
5. El código evita supuestos sobre plataformas específicas leyendo la DTB para descubrir direcciones de MMIO y número de CPUs. Cuando la DTB no existe se usa un fallback documentado.

## Organización del Código

- `cpu/boot.rs` y `cpu/boot.s`: transición desde `_start`, salto seguro a Rust y parcheo de la tabla de vectores (`VBAR_EL1`).
- `cpu/mod.rs`: expone submódulos (`scheduler`, `process`, `kernel_threads`, `userdebug_threads`). También ofrece utilidades como `ensure_el1()`.
- `cpu/scheduler.rs`: lógica del scheduler preventivo, registro de stacks por slot, handler de IRQ en ensamblador y funciones auxiliares de telemetría.
- `cpu/process.rs`: definición de `ProcessControlBlock`, `ProcessState` y tablas `KernelProcessTable`/`UserProcessTable` protegidas por `NullLock`.
- `memory/mod.rs`, `memory/page_allocator.rs`, `memory/mmu.rs`: gestor de memoria física, creación de tablas `KernelTables`, generación de `UserAddressSpace` y funciones de telemetría (`summary`, `total_free_bytes`, `kernel_ttbr1_phys`).
- `bsp/raspberrypi/*`: inicialización de periféricos, DTB, arranque de secundarios y controlador GIC-400.
- `console/mod.rs`: consola global y macro `await_kernel_uart_println!` con lock para mantener el orden de los logs.
- `panic_wait.rs`: manejador de `panic!` que imprime información y bloquea el core actual.
- `main.rs`: punto de entrada de Rust. Orquesta el arranque del BSP, inicializa el heap, la MMU, el scheduler y los núcleos secundarios.

## Gestión de Memoria del Kernel

### Flujo general de `memory::init`
1. El BSP intenta cargar la DTB real; si falla se usa un resumen de fallback con direcciones conocidas.
2. `MemoryManager::build` recopila las regiones de RAM, crea un `PageAllocator` alineado a 64 KiB y reserva el span que ocupa el binario del kernel.
3. Se aparta una reserva crítica (`EMERGENCY_POOL_BYTES = 10 MiB`) etiquetada como `ReservationKind::EmergencyPool` para stacks de kernel y emergencias OOM.
4. Se construyen tablas `KernelTables` e identidad para toda la RAM reportada y para las ventanas MMIO necesarias (UART, GPIO, SPI, GIC, periféricos locales). Las direcciones se obtienen de la DTB.
5. Se crean `UserAddressSpace` para cada slot de usuario (`scheduler::MAX_USER_TASKS = 3`). Cada espacio mapea DRAM y MMIO en modo `UserReadWrite` y publica su `ttbr0_phys` mediante `scheduler::set_user_task_ttbr`.
6. Se almacena la estructura final dentro de `MEMORY_MANAGER` (un `NullLock<Option<MemoryManager>>`) y se imprime un resumen con las identidades más relevantes.

### PageAllocator y reservas críticas
- `PageAllocator::from_regions` divide la RAM disponible en segmentos (`PageSegment`) alineados.
- `reserve_span` se usa para marcar como ocupados el binario del kernel, la DTB y cualquier otro artefacto conocido.
- `allocate_contiguous` recibe una cantidad de páginas (granulo 64 KiB) y, si hay espacio, crea un `ReservedRegion`. Se usa para el pool de emergencia.
- El método `total_free_bytes()` sirve para telemetría y para decidir si es seguro mapear nuevos espacios de usuario.

### Tablas `KernelTables` y atributos MAIR/TCR/SCTLR
- `KernelTables` contiene un puntero al nivel raíz y una lista de tablas hijas. Expone `map_identity` y `translate` para depuración.
- `memory::init` fija `MAIR_EL1` en `0x0000_0000_0000_FF04`, lo que define el índice 0 como Device nGnRE y el índice 1 como Normal WBWA.
- `TCR_EL1` se programa con granulo de 64 KiB para ambos TTBR, tamaño virtual de 48 bits y ASID de 16 bits. Se activa `EPD0` hasta que haya tablas de usuario reales.
- `SCTLR_EL1` se configura con `M|C|I = 1` y los mismos bits de seguridad que Linux (PAN especulativo, IESB, etc.).
- `memory::enable_mmu_on_this_core()` lee `kernel_ttbr1_phys()` y llama a `mmu::enable_kernel_mmu(ttbr1)` para cada núcleo.

### Espacios de usuario y `set_user_task_ttbr`
- Cada `UserAddressSpace` se construye con los mismos atributos que el kernel pero con permisos `UserReadWrite`.
- Al finalizar la creación de cada espacio se invoca `scheduler::set_user_task_ttbr(slot, ttbr0)` para que el scheduler pueda cargar el TTBR correspondiente cuando programe un `USER_TASK`.
- Mientras no existan procesos reales, estos TTBR apuntan a espacios identitarios que solo habilitan los experimentos de cooperativo.

### Telemetría: `MemorySummary`, `kernel_ttbr1_phys` y `enable_mmu_on_this_core`
- `memory::summary()` devuelve `MemorySummary`, con la lista de regiones RAM, reservas, rango del kernel, mmio_base y bytes libres.
- `memory::kernel_ttbr1_phys()` es usado por el MMU init y por el código de depuración del scheduler para verificar que el TTBR coincide con lo programado.
- `memory::enable_mmu_on_this_core()` se invoca en el boot core y en cada núcleo secundario justo antes de instalar la tabla de vectores y habilitar interrupciones.

## Scheduler Preventivo Multicore

### Componentes principales
- `MAX_CORES = 4`, `MAX_KERNEL_TASKS = 2`, `MAX_USER_TASKS = 3`.
- `KERNEL_TASKS` y `USER_TASKS` son arreglos de punteros a funciones (`kernel_task0/1`, `user_task0/1/2`) que, por ahora, ejecutan bucles infinitos de diagnóstico y `wfi()` tras habilitar IRQs.
- Cada slot tiene un stack estático de 4 KiB (`KERNEL_STACK_STACKS`, `USER_TASK_STACKS`).
- `CORE_RUNNING_SLOT` mantiene el índice asignado a cada core. `USER_TASK_TTBR0` almacena el TTBR que debe cargarse antes de restaurar un contexto de usuario.
- `ExceptionFrame` y `ContextFrame` comparten layout para hacer memcpy directo entre la pila y el PCB.

### Inicialización por core
- `kernel_init()` configura el heap, invoca `bsp::init()` y luego `memory::init()` + `memory::enable_mmu_on_this_core()`.
- `kernel_init_scheduler()` ejecuta: `bsp::init_primary_interrupts()`, `scheduler::init_kernel_process_descriptors()`, `scheduler::init_user_process_descriptors()`, `scheduler::install_vector_table()`, `scheduler::log_irq_vector_slot("boot")`, `scheduler::setup_generic_timer_5ms()` y `scheduler::enable_irq()`.
- Cada núcleo secundario llama a `memory::enable_mmu_on_this_core()`, `bsp::init_secondary_interrupts()`, vuelve a instalar la tabla de vectores y arma su temporizador antes de entrar a `userdebug_threads::run_debug_checks()`.

### Flujo de `scheduler_irq_handler`
1. La IRQ llega al vector EL1h parcheado por `boot.s`. El handler en ensamblador empuja x0-x30, SP, ELR y SPSR en la pila (ExceptionFrame).
2. Se leen los registros del GIC (IAR) y del temporizador para generar telemetría (`scheduler_irq_probe`, `log_irq_handler_entry`).
3. Se llama a `scheduler_preempt_tick(core_id, frame)` en Rust. Esta función guarda el contexto en el PCB actual, decide el siguiente slot y devuelve un puntero al `ContextFrame` a restaurar.
4. El handler reprograma el timer (5 ms), carga el nuevo SP/ELR/SPSR, restaura los registros y escribe EOIR/DIR en el GIC antes de ejecutar `eret`.
5. Se mantiene un `IRQ_HEARTBEAT` (`AtomicU64`) que almacena el último `CNTPCT_EL0` observado para facilitar la depuración.

### `scheduler_preempt_tick` y PCBs
- Si el core es 0 se ejecuta `handle_kernel_tick()`, que guarda el contexto del slot en ejecución (si existía), marca el PCB como `Assigned`, selecciona el siguiente slot (`advance_slot`) y devuelve su `ContextFrame`.
- Para cores >0 se ejecuta `handle_user_tick()`, que además de los pasos anteriores programa el TTBR del usuario (`configure_user_ttbr`) y marca el PCB como `Running`.
- Las funciones `save_kernel_context()/save_user_context()` copian registros al PCB usando `ProcessControlBlock::context.copy_from_parts`.
- `mark_kernel_slot_running/idle` y sus equivalentes de usuario actualizan `ProcessState` y `claim_flag`.

### Estados de proceso y tablas
- `ProcessState` incluye `Created`, `Assigned`, `Running`, `Sleeping`, `WaitingSyscall`, `Zombie`, `Terminated`.
- `KernelProcessTable` contiene `KERNEL_MAX_PROCESSES = 3` entradas. Cada una se inicializa con `register_kernel_task`, que asigna PID, prioridad, stack y SPSR.
- `UserProcessTable` contiene `USER_MAX_PROCESSES = 5` entradas. `register_user_task` completa TTBR0, stack de usuario y prioridades ficticias.
- Los campos `kernel_stack_base/top`, `user_stack_base/size`, `ttbr0`, `mailbox_request/response` y `claim_flag` se actualizan en tiempo real, lo que permite a los módulos de depuración imprimir su estado.

### Depuración y visibilidad
- `kernel_threads::run_debug_checks()` imprime TTBRs, SCTLR, MAIR, estado del timer y del GIC, y fuerza la habilitación de IRQ si detecta que DAIF tiene el bit I levantado.
- `userdebug_threads::run_debug_checks()` reporta el núcleo, slot, timestamp (`CNTPCT_EL0`), estado del GIC y datos básicos del PCB de usuario.
- `scheduler::log_scheduler_snapshot(label, core_id)` muestra TTBR0, slot activo y, si hay PCB asociado, su PID y prioridad.
- `await_kernel_uart_println!` envía logs de forma serializada, evitando que varios cores intercalen sus mensajes.

## Interacción con el BSP e interrupciones

### Controlador GIC-400
- `bsp/raspberrypi/interrupt_controller.rs` almacena el estado del distribuidor y la interfaz de CPU. Expone `init_primary()` y `init_secondary()` para configurar PPIs, priorizaciones y el CPU interface (`GICC_CTLR`, `GICC_PMR`, `GICC_BPR`).
- Proporciona utilidades de telemetría: `timer_irq_snapshot()`, `log_timer_irq_state(label)`, `cpu_interface_state()`, `log_cpu_interface_state(label)` y `force_timer_irq()`.
- Todas las lecturas/escrituras se hacen mediante accesos MMIO seguros (`read_volatile`/`write_volatile`) y barreras `dsb`/`isb`.

### Boot y tabla de vectores
- `boot.s` ejecuta `el2_to_el1`, reserva el DTB, estaciona los núcleos secundarios en un bucle `wfe` y, antes de saltar a Rust, parchea todas las entradas IRQ de la tabla de vectores (`VBAR_EL1`).
- Cada slot de 0x80 bytes queda apuntando a `scheduler_irq_handler`. Después del parcheo se limpia la caché de instrucciones (`ic iallu`).

### Consola y macros
- `console/mod.rs` mantiene un puntero al driver activo del BSP. El macro `await_kernel_uart_println!` toma un lock mientras escribe para evitar que las trazas del scheduler se mezclen.
- `panic_wait.rs` usa el mismo macro para imprimir la ubicación del pánico antes de bloquear el core con `cpu::wait_forever()`.

## Direccionamiento de Periféricos y MMIO

- `bsp/raspberrypi/dtb.rs` parsea la DTB usando la crate `fdt`. Calcula direcciones físicas normalizando las ventanas del bus (`normalize_peripheral_addr`).
- Se expone `PeripheralsLayout { mmio_start, uart_pl011, gpio, spi0, gic_distributor, gic_redistributor, local_intc }` y se guarda junto al resto del resumen (`DtbSummary`).
- Si la DTB no está disponible se usa un fallback que cubre Raspberry Pi 4 y 5. Las direcciones resultantes alimentan tanto a los drivers como al mapeo identidad de `memory::init()`.

## Conclusión

El código actual ya cuenta con un scheduler preventivo funcional para Core 0 (tareas del kernel) y para los núcleos de usuario, además de un gestor de memoria coherente con páginas de 64 KiB y telemetría detallada. A medida que se incorporen procesos reales bastará con poblar los PCBs de `UserProcessTable` y conectar las syscalls al mailbox existente; la infraestructura de MMU y scheduling descrita aquí no requiere cambios estructurales.
